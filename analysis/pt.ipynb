{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f04b2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22585d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_path = '/workspace/data/multi_cue_dataset/pt/ASD/20160120_ASD_lat__V1-0001.pt'\n",
    "pt = torch.load(pt_path, map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6bb51ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of frames: 69\n"
     ]
    }
   ],
   "source": [
    "frames = pt['frames']\n",
    "print(f\"Number of frames: {len(frames)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2eafc099",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.uint8"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9a0705e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video_dir: <class 'str'>\n",
      "frames: torch.Size([69, 3, 512, 512])\n",
      "label: <class 'int'>\n",
      "total_frames: <class 'int'>\n",
      "img_shape: <class 'tuple'>\n",
      "bbox_none_index: <class 'list'>\n",
      "optical_flow: torch.Size([69, 512, 512, 2])\n",
      "bbox: torch.Size([69, 4])\n",
      "mask: torch.Size([69, 1, 512, 512])\n",
      "keypoint: torch.Size([69, 17, 2])\n",
      "keypoint_score: torch.Size([69, 17])\n"
     ]
    }
   ],
   "source": [
    "for k, v in pt.items():\n",
    "    print(f\"{k}: {v.shape if isinstance(v, torch.Tensor) else type(v)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2da3dbd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/workspace/data/segmentation_dataset_512/fold0/val/ASD/20160120_ASD_lat__V1-0001.mp4')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt[\"video_dir\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "882c3376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt[\"optical_flow\"].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2d6c89f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbox = pt[\"bbox\"]\n",
    "bbox.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "169548e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([69, 4])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbox.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c186a8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbox.half()\n",
    "bbox.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f0cdd59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.float32, torch.float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keypoint = pt[\"keypoint\"]\n",
    "keypoint_score = pt[\"keypoint_score\"]\n",
    "keypoint.dtype, keypoint_score.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7b8728b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = pt[\"mask\"]\n",
    "mask.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ed8240a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理 video_dir: <class 'str'>\n",
      "处理 frames: <class 'torch.Tensor'>\n",
      "处理 label: <class 'int'>\n",
      "处理 total_frames: <class 'int'>\n",
      "处理 img_shape: <class 'tuple'>\n",
      "处理 bbox_none_index: <class 'list'>\n",
      "处理 optical_flow: <class 'torch.Tensor'>\n",
      "处理 bbox: <class 'torch.Tensor'>\n",
      "处理 mask: <class 'torch.Tensor'>\n",
      "处理 keypoint: <class 'torch.Tensor'>\n",
      "处理 keypoint_score: <class 'torch.Tensor'>\n",
      "\n",
      "✅ 保存前的数据结构：\n",
      "video_dir: type=str, value=/workspace/data/segmentation_dataset_512/fold0/val/ASD/20160120_ASD_lat__V1-0001.mp4\n",
      "frames: dtype=torch.uint8, shape=(69, 3, 512, 512)\n",
      "label: type=int, value=0\n",
      "total_frames: type=int, value=69\n",
      "img_shape: type=tuple, value=(512, 512)\n",
      "bbox_none_index: type=list, value=[]\n",
      "optical_flow: dtype=torch.float16, shape=(69, 512, 512, 2)\n",
      "bbox: dtype=torch.float16, shape=(69, 4)\n",
      "mask: dtype=torch.uint8, shape=(69, 1, 512, 512)\n",
      "keypoint: dtype=torch.float16, shape=(69, 17, 2)\n",
      "keypoint_score: dtype=torch.float16, shape=(69, 17)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ 保存完成: inference_result.pt.gz\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gzip\n",
    "from pathlib import Path\n",
    "\n",
    "# 假设 pt 是你的推理结果字典\n",
    "# pt = {...}\n",
    "\n",
    "# 类型转换 + 压缩准备\n",
    "for k, v in pt.items():\n",
    "    print(f\"处理 {k}: {type(v)}\")\n",
    "    \n",
    "    # 不需处理的基本类型\n",
    "    if isinstance(v, (str, int, tuple)):\n",
    "        continue\n",
    "\n",
    "    # Path 类型转为字符串\n",
    "    elif isinstance(v, Path):\n",
    "        pt[k] = str(v)\n",
    "\n",
    "    # Tensor 类型处理\n",
    "    elif isinstance(v, torch.Tensor):\n",
    "        if k == \"mask\":\n",
    "            pt[k] = v.to(torch.uint8)\n",
    "        elif v.is_floating_point() and v.dtype == torch.float32:\n",
    "            try:\n",
    "                pt[k] = v.to(torch.float16)\n",
    "            except Exception as e:\n",
    "                print(f\"警告: 无法将 {k} 转换为 float16: {e}\")\n",
    "\n",
    "# 打印处理后的数据结构\n",
    "print(\"\\n✅ 保存前的数据结构：\")\n",
    "for k, v in pt.items():\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        print(f\"{k}: dtype={v.dtype}, shape={tuple(v.shape)}\")\n",
    "    else:\n",
    "        print(f\"{k}: type={type(v).__name__}, value={v}\")\n",
    "\n",
    "# 保存为 gzip 压缩的 .pt 文件\n",
    "with gzip.open(\"inference_result.pt.gz\", \"wb\") as f:\n",
    "    torch.save(pt, f)\n",
    "\n",
    "print(\"\\n✅ 保存完成: inference_result.pt.gz\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
